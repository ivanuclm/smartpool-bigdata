{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0af9725-4451-4cc4-a1b0-1960a6304176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-af1d6aef-95a3-4365-b176-b7e02c118292;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 123ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-af1d6aef-95a3-4365-b176-b7e02c118292\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n",
      "26/01/25 19:04:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark OK: 4.0.1\n",
      "BASE: s3a://spark/medallion\n",
      "BRONZE: s3a://spark/medallion/bronze\n",
      "SILVER: s3a://spark/medallion/silver\n",
      "GOLD: s3a://spark/medallion/gold\n",
      "STATE: s3a://spark/medallion/_state\n",
      "JDBC: jdbc:sqlserver://sqlserver:1433;databaseName=smartpool;encrypt=true;trustServerCertificate=true;\n"
     ]
    }
   ],
   "source": [
    "from smartpool_config import *\n",
    "\n",
    "spark = create_spark(\"smartpool-ingestion\")\n",
    "\n",
    "print(\"Spark OK:\", spark.version)\n",
    "print(\"BASE:\", BASE)\n",
    "print(\"BRONZE:\", BRONZE)\n",
    "print(\"SILVER:\", SILVER)\n",
    "print(\"GOLD:\", GOLD)\n",
    "print(\"STATE:\", STATE)\n",
    "print(\"JDBC:\", JDBC_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ddab08-46d8-4af7-b465-e3cd7587bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jdbc_read(query: str):\n",
    "    return (\n",
    "        spark.read.format(\"jdbc\")\n",
    "        .option(\"url\", JDBC_URL)\n",
    "        .option(\"query\", query)\n",
    "        .option(\"user\", JDBC_USER)\n",
    "        .option(\"password\", JDBC_PASS)\n",
    "        .option(\"driver\", JDBC_DRIVER)\n",
    "        .load()\n",
    "    )\n",
    "\n",
    "def state_path(table: str) -> str:\n",
    "    return f\"{STATE}/{table}\"\n",
    "\n",
    "def read_watermark(table: str):\n",
    "    \"\"\"\n",
    "    Devuelve (last_updated_at_str, last_pk).\n",
    "    last_updated_at_str en formato ISO con 7 decimales: 2026-01-25T18:37:37.8426983\n",
    "    \"\"\"\n",
    "    p = state_path(table)\n",
    "    if DeltaTable.isDeltaTable(spark, p):\n",
    "        r = spark.read.format(\"delta\").load(p).limit(1).collect()[0]\n",
    "        return r[\"last_updated_at_str\"], int(r[\"last_pk\"])\n",
    "    return None, 0\n",
    "\n",
    "def write_watermark(table: str, last_updated_at_str: str, last_pk: int):\n",
    "    p = state_path(table)\n",
    "    df_state = spark.createDataFrame(\n",
    "        [(last_updated_at_str, int(last_pk))],\n",
    "        [\"last_updated_at_str\", \"last_pk\"]\n",
    "    )\n",
    "    (\n",
    "        df_state.write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .save(p)\n",
    "    )\n",
    "    print(f\"[state:{table}] last_updated_at_str={last_updated_at_str}, last_pk={last_pk} -> {p}\")\n",
    "\n",
    "def ingest_incremental(table: str, pk: str, updated_col: str = \"updated_at\"):\n",
    "    bronze_path = f\"{BRONZE}/{table}\"\n",
    "    last_ts_str, last_pk = read_watermark(table)\n",
    "\n",
    "    if last_ts_str is None:\n",
    "        print(f\"[{table}] No hay state, FULL LOAD inicial\")\n",
    "        where = \"1=1\"\n",
    "    else:\n",
    "        print(f\"[{table}] State detectado, last_updated_at_str={last_ts_str}, last_pk={last_pk}\")\n",
    "        where = f\"\"\"\n",
    "            {updated_col} > CAST('{last_ts_str}' AS datetime2(7))\n",
    "            OR ({updated_col} = CAST('{last_ts_str}' AS datetime2(7)) AND {pk} > {last_pk})\n",
    "        \"\"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            *,\n",
    "            CONVERT(varchar(33), {updated_col}, 126) AS {updated_col}_str\n",
    "        FROM dbo.{table}\n",
    "        WHERE {where}\n",
    "    \"\"\"\n",
    "\n",
    "    df_new = jdbc_read(query)\n",
    "    cnt = df_new.count()\n",
    "    print(f\"[{table}] Nuevas filas leídas: {cnt}\")\n",
    "\n",
    "    if cnt == 0:\n",
    "        print(f\"[{table}] Nada que cargar, fin.\")\n",
    "        return\n",
    "\n",
    "    # Append Bronze (raw incremental)\n",
    "    (\n",
    "        df_new.drop(f\"{updated_col}_str\")\n",
    "        .write.format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .save(bronze_path)\n",
    "    )\n",
    "    print(f\"[{table}] Append Bronze OK -> {bronze_path}\")\n",
    "\n",
    "    # Watermark exacto (string con 7 decimales) + tie-breaker por PK\n",
    "    max_ts_str = df_new.agg(F.max(f\"{updated_col}_str\").alias(\"max_ts\")).collect()[0][\"max_ts\"]\n",
    "    max_pk = (\n",
    "        df_new.filter(F.col(f\"{updated_col}_str\") == F.lit(max_ts_str))\n",
    "        .agg(F.max(pk).alias(\"max_pk\"))\n",
    "        .collect()[0][\"max_pk\"]\n",
    "    )\n",
    "    write_watermark(table, max_ts_str, int(max_pk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f25e37d3-c66d-4c82-b691-9dbe4d3ef619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pools_dim] State detectado, last_updated_at_str=2026-01-25T19:07:16.2612979, last_pk=6\n",
      "[pools_dim] Nuevas filas leídas: 0\n",
      "[pools_dim] Nada que cargar, fin.\n"
     ]
    }
   ],
   "source": [
    "ingest_incremental(table=\"pools_dim\", pk=\"pool_id\", updated_col=\"updated_at\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3ee50e5-4484-4bb9-964e-fa9bc2ee72cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[maintenance_events] State detectado, last_updated_at_str=2026-01-25T01:10:41.8562930, last_pk=14\n",
      "[maintenance_events] Nuevas filas leídas: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[maintenance_events] Append Bronze OK -> s3a://spark/medallion/bronze/maintenance_events\n",
      "[state:maintenance_events] last_updated_at_str=2026-01-25T19:07:21.8205643, last_pk=1003 -> s3a://spark/medallion/_state/maintenance_events\n"
     ]
    }
   ],
   "source": [
    "ingest_incremental(table=\"maintenance_events\", pk=\"id\", updated_col=\"updated_at\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2bd07190-fc18-44eb-8a25-e0b2563d7e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BRONZE pools_dim ===\n",
      "+-------+--------------------+--------------------+-------------+---------+-------------+--------------------+\n",
      "|pool_id|           pool_name|            location|volume_liters|is_heated|   owner_type|          updated_at|\n",
      "+-------+--------------------+--------------------+-------------+---------+-------------+--------------------+\n",
      "|      1| Piscina Casa Pueblo|Valdeganga (Albac...|        40000|    false|      private|2026-01-25 18:37:...|\n",
      "|      2|  Piscina Villa Mila|Valdeganga (Albac...|        70000|    false|      private|2026-01-25 18:37:...|\n",
      "|      3|Piscina Airbnb Rural|              Cuenca|        35000|    false|       airbnb|2026-01-25 18:37:...|\n",
      "|      4|Piscina Hotel Centro|              Madrid|        60000|     true|        hotel|2026-01-25 18:37:...|\n",
      "|      5|Piscina Polidepor...|         Ciudad Real|        80000|     true|sports_center|2026-01-25 18:37:...|\n",
      "|      6|Piscina Prueba In...|            Albacete|        42000|     true|      private|2026-01-25 19:07:...|\n",
      "+-------+--------------------+--------------------+-------------+---------+-------------+--------------------+\n",
      "\n",
      "=== STATE pools_dim ===\n",
      "+---------------------------+-------+\n",
      "|last_updated_at_str        |last_pk|\n",
      "+---------------------------+-------+\n",
      "|2026-01-25T19:07:16.2612979|6      |\n",
      "+---------------------------+-------+\n",
      "\n",
      "=== BRONZE maintenance_events ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------------+-----------------+------------+--------------+--------------------+--------------------+\n",
      "|  id|pool_id|          event_time|intervention_type|product_type|product_amount|               notes|          updated_at|\n",
      "+----+-------+--------------------+-----------------+------------+--------------+--------------------+--------------------+\n",
      "|   1|      1|2026-01-15 01:10:...|         chlorine|    dichloro|         250.0|Tratamiento de ch...|2026-01-25 01:10:...|\n",
      "|   2|      1|2026-01-18 01:10:...|    ph_correction|       minus|         150.0|Ajuste de pH tras...|2026-01-25 01:10:...|\n",
      "|   3|      1|2026-01-22 01:10:...|  filter_backwash|        NULL|          NULL|Lavado de filtro ...|2026-01-25 01:10:...|\n",
      "|   4|      2|2026-01-17 01:10:...|         chlorine|    tricloro|         200.0|Mantenimiento rut...|2026-01-25 01:10:...|\n",
      "|   5|      2|2026-01-21 01:10:...|           refill|        NULL|        1500.0|Relleno por evapo...|2026-01-25 01:10:...|\n",
      "|   6|      2|2026-01-24 01:10:...|    ph_correction|        plus|         120.0|Corrección de pH ...|2026-01-25 01:10:...|\n",
      "|   7|      3|2026-01-19 01:10:...|         chlorine|    dichloro|         220.0|Preparación antes...|2026-01-25 01:10:...|\n",
      "|   8|      3|2026-01-23 01:10:...|  filter_backwash|        NULL|          NULL|Lavado de filtro ...|2026-01-25 01:10:...|\n",
      "|   9|      4|2026-01-16 01:10:...|         chlorine|    tricloro|         300.0|Mantenimiento sem...|2026-01-25 01:10:...|\n",
      "|  10|      4|2026-01-20 01:10:...|           refill|        NULL|        2500.0|Relleno por pérdi...|2026-01-25 01:10:...|\n",
      "|  11|      4|2026-01-23 01:10:...|    ph_correction|       minus|         180.0|Ajuste de pH por ...|2026-01-25 01:10:...|\n",
      "|  12|      5|2026-01-18 01:10:...|         chlorine|    dichloro|         400.0|Tratamiento de ch...|2026-01-25 01:10:...|\n",
      "|  13|      5|2026-01-22 01:10:...|  filter_backwash|        NULL|          NULL|Lavado de filtro ...|2026-01-25 01:10:...|\n",
      "|  14|      5|2026-01-24 01:10:...|    ph_correction|       minus|         200.0|Corrección de pH ...|2026-01-25 01:10:...|\n",
      "|1002|      1|2026-01-25 19:07:...|         chlorine|    dichloro|         180.0|Evento incrementa...|2026-01-25 19:07:...|\n",
      "|1003|      1|2026-01-25 19:09:...|    ph_correction|       minus|          90.0|Evento incrementa...|2026-01-25 19:07:...|\n",
      "+----+-------+--------------------+-----------------+------------+--------------+--------------------+--------------------+\n",
      "\n",
      "=== STATE maintenance_events ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------+\n",
      "|last_updated_at_str        |last_pk|\n",
      "+---------------------------+-------+\n",
      "|2026-01-25T19:07:21.8205643|1003   |\n",
      "+---------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=== BRONZE pools_dim ===\")\n",
    "spark.read.format(\"delta\").load(f\"{BRONZE}/pools_dim\").show()\n",
    "print(\"=== STATE pools_dim ===\")\n",
    "spark.read.format(\"delta\").load(f\"{STATE}/pools_dim\").show(truncate=False)\n",
    "\n",
    "print(\"=== BRONZE maintenance_events ===\")\n",
    "spark.read.format(\"delta\").load(f\"{BRONZE}/maintenance_events\").show()\n",
    "print(\"=== STATE maintenance_events ===\")\n",
    "spark.read.format(\"delta\").load(f\"{STATE}/maintenance_events\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed791a6f-d5eb-4c90-8a7a-886229c1774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
