{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0af9725-4451-4cc4-a1b0-1960a6304176",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2.5.2/jars\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b0779132-aeb9-4591-9e6a-0c418b228499;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-spark_2.13;4.0.0 in central\n",
      "\tfound io.delta#delta-storage;4.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      ":: resolution report :: resolve 119ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-spark_2.13;4.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b0779132-aeb9-4591-9e6a-0c418b228499\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n",
      "26/01/25 20:06:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark OK: 4.0.1\n",
      "BASE: s3a://spark/medallion\n",
      "BRONZE: s3a://spark/medallion/bronze\n",
      "SILVER: s3a://spark/medallion/silver\n",
      "GOLD: s3a://spark/medallion/gold\n",
      "STATE: s3a://spark/medallion/_state\n",
      "JDBC: jdbc:sqlserver://sqlserver:1433;databaseName=smartpool;encrypt=true;trustServerCertificate=true;\n"
     ]
    }
   ],
   "source": [
    "from smartpool_config import *\n",
    "\n",
    "spark = create_spark(\"smartpool-silver\")\n",
    "\n",
    "print(\"Spark OK:\", spark.version)\n",
    "print(\"BASE:\", BASE)\n",
    "print(\"BRONZE:\", BRONZE)\n",
    "print(\"SILVER:\", SILVER)\n",
    "print(\"GOLD:\", GOLD)\n",
    "print(\"STATE:\", STATE)\n",
    "print(\"JDBC:\", JDBC_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d17e7c0d-4c2e-4902-b221-19135e3c931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POOLS_SILVER : s3a://spark/medallion/silver/pools_dim\n",
      "EVENTS_SILVER: s3a://spark/medallion/silver/maintenance_events\n",
      "GOLD_DAILY   : s3a://spark/medallion/gold/pool_daily_metrics\n",
      "GOLD_LATEST  : s3a://spark/medallion/gold/pool_latest_event\n"
     ]
    }
   ],
   "source": [
    "POOLS_BRONZE = f\"{BRONZE}/pools_dim\"\n",
    "EVENTS_BRONZE = f\"{BRONZE}/maintenance_events\"\n",
    "\n",
    "POOLS_SILVER = f\"{SILVER}/pools_dim\"\n",
    "EVENTS_SILVER = f\"{SILVER}/maintenance_events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "640bc228-428f-45a2-96bd-5910d508ff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_exists(path: str) -> bool:\n",
    "    try:\n",
    "        return DeltaTable.isDeltaTable(spark, path)\n",
    "    except Exception:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56da31f9-bcb6-4f10-bd59-85cd5da4c506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SILVER:pools_dim] OK -> s3a://spark/medallion/silver/pools_dim rows: 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[pool_id: int, pool_name: string, location: string, volume_liters: int, is_heated: boolean, owner_type: string, updated_at: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bronze_pools = spark.read.format(\"delta\").load(POOLS_BRONZE)\n",
    "\n",
    "# Normaliza tipos (por si acaso)\n",
    "pools = (bronze_pools\n",
    "    .select(\n",
    "        F.col(\"pool_id\").cast(\"int\").alias(\"pool_id\"),\n",
    "        F.col(\"pool_name\").cast(\"string\").alias(\"pool_name\"),\n",
    "        F.col(\"location\").cast(\"string\").alias(\"location\"),\n",
    "        F.col(\"volume_liters\").cast(\"int\").alias(\"volume_liters\"),\n",
    "        F.col(\"is_heated\").cast(\"boolean\").alias(\"is_heated\"),\n",
    "        F.col(\"owner_type\").cast(\"string\").alias(\"owner_type\"),\n",
    "        F.col(\"updated_at\").cast(\"timestamp\").alias(\"updated_at\"),\n",
    "    )\n",
    "    .filter(F.col(\"pool_id\").isNotNull())\n",
    ")\n",
    "\n",
    "# Última versión por pool_id (si empatan timestamps, desempata por pool_id)\n",
    "w = Window.partitionBy(\"pool_id\").orderBy(F.col(\"updated_at\").desc(), F.col(\"pool_id\").desc())\n",
    "\n",
    "pools_latest = (pools\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "(pools_latest.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .save(POOLS_SILVER)\n",
    ")\n",
    "\n",
    "print(\"[SILVER:pools_dim] OK ->\", POOLS_SILVER, \"rows:\", pools_latest.count())\n",
    "display(pools_latest.orderBy(\"pool_id\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9beee1f-177d-4f11-b5f7-df94253a0c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/25 20:12:11 WARN MapPartitionsRDD: RDD 385 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SILVER:maintenance_events] MERGE OK -> s3a://spark/medallion/silver/maintenance_events rows(updates dataset): 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[pool_id: int, id: int, event_time: timestamp, intervention_type: string, product_type: string, product_amount: double, notes: string, updated_at: timestamp, event_date: date]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bronze_events = spark.read.format(\"delta\").load(EVENTS_BRONZE)\n",
    "\n",
    "# Curación + tipos\n",
    "events = (bronze_events\n",
    "    .select(\n",
    "        F.col(\"id\").cast(\"int\").alias(\"id\"),\n",
    "        F.col(\"pool_id\").cast(\"int\").alias(\"pool_id\"),\n",
    "        F.col(\"event_time\").cast(\"timestamp\").alias(\"event_time\"),\n",
    "        F.col(\"intervention_type\").cast(\"string\").alias(\"intervention_type\"),\n",
    "        F.col(\"product_type\").cast(\"string\").alias(\"product_type\"),\n",
    "        F.col(\"product_amount\").cast(\"double\").alias(\"product_amount\"),\n",
    "        F.col(\"notes\").cast(\"string\").alias(\"notes\"),\n",
    "        F.col(\"updated_at\").cast(\"timestamp\").alias(\"updated_at\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Data quality mínima (puedes ampliar)\n",
    "allowed = [\"chlorine\", \"refill\", \"ph_correction\", \"filter_backwash\"]\n",
    "events = (events\n",
    "    .filter(F.col(\"id\").isNotNull())\n",
    "    .filter(F.col(\"pool_id\").isNotNull())\n",
    "    .filter(F.col(\"event_time\").isNotNull())\n",
    "    .filter(F.col(\"intervention_type\").isNotNull())\n",
    "    .filter(F.col(\"intervention_type\").isin(allowed))\n",
    ")\n",
    "\n",
    "# Enriquecimiento útil para particionar y queries\n",
    "events = events.withColumn(\"event_date\", F.to_date(\"event_time\"))\n",
    "\n",
    "# Integridad referencial: solo pools que existan en Silver pools_dim\n",
    "silver_pools = spark.read.format(\"delta\").load(POOLS_SILVER).select(\"pool_id\").dropDuplicates([\"pool_id\"])\n",
    "events = events.join(silver_pools, on=\"pool_id\", how=\"inner\")\n",
    "\n",
    "# Dedup por id: última versión por updated_at (y desempate por id)\n",
    "w = Window.partitionBy(\"id\").orderBy(F.col(\"updated_at\").desc(), F.col(\"id\").desc())\n",
    "events_latest = (events\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\") == 1)\n",
    "    .drop(\"rn\")\n",
    ")\n",
    "\n",
    "# Crea tabla Silver si no existe (particionada por event_date)\n",
    "if not delta_exists(EVENTS_SILVER):\n",
    "    (events_latest.limit(0).write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .partitionBy(\"event_date\")\n",
    "        .save(EVENTS_SILVER)\n",
    "    )\n",
    "    print(\"[SILVER:maintenance_events] creada ->\", EVENTS_SILVER)\n",
    "\n",
    "tgt = DeltaTable.forPath(spark, EVENTS_SILVER)\n",
    "\n",
    "(tgt.alias(\"t\")\n",
    "    .merge(events_latest.alias(\"s\"), \"t.id = s.id\")\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "print(\"[SILVER:maintenance_events] MERGE OK ->\", EVENTS_SILVER, \"rows(updates dataset):\", events_latest.count())\n",
    "display(events_latest.orderBy(F.col(\"updated_at\").desc()).limit(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "becfaa4f-cbc9-4f71-81a6-d6cf4f40b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SILVER pools rows: 6\n",
      "SILVER events rows: 16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[pool_id: int, id: int, event_time: timestamp, intervention_type: string, product_type: string, product_amount: double, notes: string, updated_at: timestamp, event_date: date]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"SILVER pools rows:\", spark.read.format(\"delta\").load(POOLS_SILVER).count())\n",
    "print(\"SILVER events rows:\", spark.read.format(\"delta\").load(EVENTS_SILVER).count())\n",
    "\n",
    "display(spark.read.format(\"delta\").load(EVENTS_SILVER).orderBy(F.col(\"event_time\").desc()).limit(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9313ff47-d0b5-4238-be16-0771f2c300ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SILVER pools: s3a://spark/medallion/silver/pools_dim rows: 6\n",
      "SILVER events: s3a://spark/medallion/silver/maintenance_events rows: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/25 20:12:23 ERROR TaskSchedulerImpl: Lost executor 1 on 172.20.0.8: Command exited with code 137\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_1!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_11!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_1!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_8!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_36!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_45!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_7!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_39!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_46!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_5!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_28!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_46!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_6!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_16!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_29!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_41!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_5!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_24!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_11!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_21!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_41!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_35!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_35!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_1!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_6!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_5!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_41!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_3!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_45!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_16!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_24!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_39!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_18!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_4!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_26!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_43!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_46!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_39!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_45!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_32!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_9!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_19!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_24!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_25!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_33!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_35!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_8!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_15!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_30!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_28!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_11!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_42!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_23!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_28!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_28!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_36!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_5!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_16!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_5!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_46!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_31!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_36!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_43!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_29!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_18!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_7!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_30!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_40!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_19!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_20!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_20!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_16!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_20!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_46!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_34!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_34!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_19!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_25!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_45!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_42!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_33!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_12!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_7!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_20!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_17!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_12!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_21!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_24!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_37!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_13!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_15!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_17!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_23!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_40!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_3!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_36!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_49!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_13!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_31!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_7!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_37!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_18!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_4!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_5!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_43!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_7!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_24!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_7!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_42!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_16!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_23!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_9!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_40!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_41!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_15!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_12!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_8!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_20!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_46!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_49!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_10!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_15!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_36!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_27!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_46!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_40!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_47!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_13!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_25!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_1!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_36!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_1!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_46!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_39!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_40!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_5!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_38!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_28!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_24!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_39!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_35!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_40!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_29!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_43!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_47!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_32!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_36!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_43!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_23!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_29!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_44!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_15!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_12!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_37!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_16!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_33!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_8!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_11!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_4!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_26!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_42!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_12!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_9!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_31!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_23!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_38!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_13!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_42!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_24!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_36!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_15!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_47!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_38!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_19!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_43!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_23!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_1!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_25!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_18!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_38!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_33!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_24!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_11!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_39!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_11!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_32!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_2!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_4!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_365_43!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_37!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_32!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_44!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_32!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_457_10!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_10_5!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_9!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_42!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_20!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_77_47!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_12!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_31!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_73_28!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_27!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_461_43!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_14_32!\n",
      "26/01/25 20:12:23 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_361_2!\n",
      "26/01/25 20:12:23 WARN TaskSetManager: Lost task 0.0 in stage 215.0 (TID 1863) (172.20.0.9 executor 0): FetchFailed(null, shuffleId=53, mapIndex=-1, mapId=-1, reduceId=0, message=\n",
      "org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 53 partition 0\n",
      "\tat org.apache.spark.MapOutputTracker$.validateStatus(MapOutputTracker.scala:1770)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11(MapOutputTracker.scala:1715)\n",
      "\tat org.apache.spark.MapOutputTracker$.$anonfun$convertMapStatuses$11$adapted(MapOutputTracker.scala:1714)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n",
      "\tat org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:1714)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorIdImpl(MapOutputTracker.scala:1348)\n",
      "\tat org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:1310)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:135)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader(ShuffleManager.scala:67)\n",
      "\tat org.apache.spark.shuffle.ShuffleManager.getReader$(ShuffleManager.scala:61)\n",
      "\tat org.apache.spark.shuffle.sort.SortShuffleManager.getReader(SortShuffleManager.scala:73)\n",
      "\tat org.apache.spark.sql.execution.ShuffledRowRDD.compute(ShuffledRowRDD.scala:200)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:374)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:338)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:107)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      ")\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Unicidad OK (pool_id, id)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Not-null críticos OK\n",
      "[QA] Catálogo intervention_type OK\n",
      "[QA] FK events.pool_id -> pools_dim.pool_id OK\n",
      "[QA] Partition columns EVENTS_SILVER: ['event_date']\n",
      "[QA] Particionado OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[pool_id: int, pool_name: string, location: string, volume_liters: int, is_heated: boolean, owner_type: string, updated_at: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[pool_id: int, id: int, event_time: timestamp, intervention_type: string, product_type: string, product_amount: double, notes: string, updated_at: timestamp, event_date: date]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Delta history pools_dim\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QA] Delta history maintenance_events\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QA FINAL OK ✅\n"
     ]
    }
   ],
   "source": [
    "# ===============\n",
    "# QA / VALIDACIONES\n",
    "# ===============\n",
    "\n",
    "pools_s = spark.read.format(\"delta\").load(POOLS_SILVER)\n",
    "events_s = spark.read.format(\"delta\").load(EVENTS_SILVER)\n",
    "\n",
    "print(\"SILVER pools:\", POOLS_SILVER, \"rows:\", pools_s.count())\n",
    "print(\"SILVER events:\", EVENTS_SILVER, \"rows:\", events_s.count())\n",
    "\n",
    "# 1) Unicidad de claves en Silver\n",
    "dup_pools = (pools_s.groupBy(\"pool_id\").count().filter(\"count > 1\").count())\n",
    "dup_events = (events_s.groupBy(\"id\").count().filter(\"count > 1\").count())\n",
    "\n",
    "if dup_pools != 0:\n",
    "    raise Exception(f\"[QA] pools_dim tiene duplicados por pool_id: {dup_pools}\")\n",
    "if dup_events != 0:\n",
    "    raise Exception(f\"[QA] maintenance_events tiene duplicados por id: {dup_events}\")\n",
    "\n",
    "print(\"[QA] Unicidad OK (pool_id, id)\")\n",
    "\n",
    "# 2) Nulos críticos\n",
    "null_pools = pools_s.filter(\"pool_id IS NULL OR pool_name IS NULL OR updated_at IS NULL\").count()\n",
    "null_events = events_s.filter(\"id IS NULL OR pool_id IS NULL OR event_time IS NULL OR intervention_type IS NULL OR updated_at IS NULL\").count()\n",
    "\n",
    "if null_pools != 0:\n",
    "    raise Exception(f\"[QA] pools_dim tiene nulos críticos: {null_pools}\")\n",
    "if null_events != 0:\n",
    "    raise Exception(f\"[QA] maintenance_events tiene nulos críticos: {null_events}\")\n",
    "\n",
    "print(\"[QA] Not-null críticos OK\")\n",
    "\n",
    "# 3) Dominio de intervention_type (por si se cuela algo)\n",
    "allowed = set([\"chlorine\", \"refill\", \"ph_correction\", \"filter_backwash\"])\n",
    "bad_types = events_s.filter(~F.col(\"intervention_type\").isin(list(allowed))).count()\n",
    "if bad_types != 0:\n",
    "    raise Exception(f\"[QA] intervention_type fuera de catálogo: {bad_types}\")\n",
    "print(\"[QA] Catálogo intervention_type OK\")\n",
    "\n",
    "# 4) Integridad referencial: todos los pool_id de events deben existir en pools_dim\n",
    "missing_fk = (\n",
    "    events_s.select(\"pool_id\").distinct()\n",
    "    .join(pools_s.select(\"pool_id\").distinct(), on=\"pool_id\", how=\"left_anti\")\n",
    "    .count()\n",
    ")\n",
    "if missing_fk != 0:\n",
    "    raise Exception(f\"[QA] Hay pool_id en maintenance_events que no existen en pools_dim: {missing_fk}\")\n",
    "\n",
    "print(\"[QA] FK events.pool_id -> pools_dim.pool_id OK\")\n",
    "\n",
    "# 5) Validación de particionado real (debería ser event_date)\n",
    "detail = spark.sql(f\"DESCRIBE DETAIL delta.`{EVENTS_SILVER}`\").collect()[0]\n",
    "print(\"[QA] Partition columns EVENTS_SILVER:\", detail[\"partitionColumns\"])\n",
    "if \"event_date\" not in detail[\"partitionColumns\"]:\n",
    "    raise Exception(\"[QA] EVENTS_SILVER no está particionada por event_date (revisa .partitionBy('event_date'))\")\n",
    "\n",
    "print(\"[QA] Particionado OK\")\n",
    "\n",
    "# 6) “Smoke test” rápido: top-N para inspección visual\n",
    "display(pools_s.orderBy(F.col(\"updated_at\").desc(), F.col(\"pool_id\")).limit(20))\n",
    "display(events_s.orderBy(F.col(\"updated_at\").desc(), F.col(\"id\")).limit(50))\n",
    "\n",
    "# 7) Historial Delta (útil para demostrar MERGE/overwrite)\n",
    "print(\"[QA] Delta history pools_dim\")\n",
    "display(DeltaTable.forPath(spark, POOLS_SILVER).history(10))\n",
    "print(\"[QA] Delta history maintenance_events\")\n",
    "display(DeltaTable.forPath(spark, EVENTS_SILVER).history(10))\n",
    "\n",
    "print(\"QA FINAL OK ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0787e2ff-8736-4479-b7fd-228fd1f6b28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[pool_id: int, num_events: bigint, last_event_time: timestamp]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Métrica simple: nº eventos por piscina y último evento\n",
    "events_s = spark.read.format(\"delta\").load(EVENTS_SILVER)\n",
    "agg = (events_s\n",
    "    .groupBy(\"pool_id\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"num_events\"),\n",
    "        F.max(\"event_time\").alias(\"last_event_time\")\n",
    "    )\n",
    "    .orderBy(F.col(\"num_events\").desc())\n",
    ")\n",
    "display(agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "757d7fb6-bea2-453c-b4a3-06d0d5b90333",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19de543d-d762-4a0a-afbb-612bee41171b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
