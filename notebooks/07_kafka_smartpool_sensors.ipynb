{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9428d36-65ed-48e4-8ccd-d1242696cca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2.5.2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2.5.2/jars\n",
      "org.apache.spark#spark-token-provider-kafka-0-10_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8c7b02a5-ab43-4ff8-9369-92694265820c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.9.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      ":: resolution report :: resolve 237ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.9.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.13;4.0.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   8   |   0   |   0   |   0   ||   8   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8c7b02a5-ab43-4ff8-9369-92694265820c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 8 already retrieved (0kB/6ms)\n",
      "26/01/26 22:45:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, DoubleType, TimestampType, DateType, LongType, BooleanType\n",
    ")\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# =========\n",
    "# MINIO / S3A\n",
    "# =========\n",
    "MINIO_ENDPOINT = \"http://minio:9000\"\n",
    "MINIO_ACCESS_KEY = \"minioadmin\"\n",
    "MINIO_SECRET_KEY = \"minioadmin123\"\n",
    "MINIO_BUCKET = \"spark\"\n",
    "\n",
    "BASE   = f\"s3a://{MINIO_BUCKET}/medallion\"\n",
    "BRONZE = f\"{BASE}/bronze\"\n",
    "SILVER = f\"{BASE}/silver\"\n",
    "GOLD   = f\"{BASE}/gold\"\n",
    "STATE  = f\"{BASE}/_state\"\n",
    "\n",
    "JDBC_URL = (\n",
    "    \"jdbc:sqlserver://sqlserver:1433;\"\n",
    "    \"databaseName=smartpool;\"\n",
    "    \"encrypt=true;\"\n",
    "    \"trustServerCertificate=true;\"\n",
    ")\n",
    "JDBC_USER = \"sa\"\n",
    "JDBC_PASS = \"Password1234%\"\n",
    "JDBC_DRIVER = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "# =========\n",
    "# PACKAGES (Ivy/Maven)\n",
    "# =========\n",
    "\n",
    "EXTRA_JARS = \",\".join([\n",
    "    \"/opt/spark/jars/hadoop-aws-3.4.0.jar\",\n",
    "    \"/opt/spark/jars/hadoop-common-3.4.0.jar\",\n",
    "    \"/opt/spark/jars/aws-java-sdk-bundle-2.23.19.jar\",\n",
    "    \"/opt/spark/jars/mssql-jdbc-12.10.2.jre11.jar\",\n",
    "    \"/opt/spark/jars/delta-spark_2.13-4.0.0.jar\",\n",
    "    \"/opt/spark/jars/delta-storage-4.0.0.jar\",\n",
    "    \"/opt/spark/jars/antlr4-runtime-4.13.1.jar\",\n",
    "    \"/opt/spark/jars/spark-sql-kafka-0-10_2.13-4.0.1.jar\",\n",
    "    # \"/opt/spark/jars/spark-token-provider-kafka-0-10_2.13-4.0.1.jar\",\n",
    "    \"/opt/spark/jars/kafka-clients-3.9.1.jar\",\n",
    "])\n",
    "\n",
    "SPARK_PACKAGES = \",\".join([\n",
    "    # Kafka (necesitas LOS DOS para evitar KafkaConfigUpdater)\n",
    "    # \"org.apache.spark:spark-sql-kafka-0-10_2.13:4.0.1\",\n",
    "    \"org.apache.spark:spark-token-provider-kafka-0-10_2.13:4.0.1\",\n",
    "\n",
    "    # Delta\n",
    "    # \"io.delta:delta-spark_2.13:4.0.0\",\n",
    "\n",
    "    # S3A / AWS SDK (elige un combo coherente; este suele ir bien)\n",
    "    # \"org.apache.hadoop:hadoop-aws:3.4.1\",\n",
    "    # \"software.amazon.awssdk:bundle:2.24.6\",\n",
    "\n",
    "    # SQLServer JDBC (opcional; si prefieres local jar, quítalo de aquí)\n",
    "    # \"com.microsoft.sqlserver:mssql-jdbc:12.10.2.jre11\",\n",
    "])\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"smartpool-kafka-sensors\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.submit.deployMode\", \"client\")  # importante en notebooks\n",
    "\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"6\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Europe/Madrid\")\n",
    "    .config(\"spark.streaming.stopGracefullyOnShutdown\", \"true\")\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")\n",
    "\n",
    "    # Delta\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "    # MinIO / S3A\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", MINIO_ENDPOINT)\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", MINIO_ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", MINIO_SECRET_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "\n",
    "    # Dependencias por Ivy\n",
    "    .config(\"spark.jars.packages\", SPARK_PACKAGES)\n",
    "    .config(\"spark.jars\", EXTRA_JARS)\n",
    "    \n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0af9725-4451-4cc4-a1b0-1960a6304176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark OK: 4.0.1\n",
      "BASE: s3a://spark/medallion\n",
      "BRONZE: s3a://spark/medallion/bronze\n",
      "SILVER: s3a://spark/medallion/silver\n",
      "GOLD: s3a://spark/medallion/gold\n",
      "STATE: s3a://spark/medallion/_state\n",
      "JDBC: jdbc:sqlserver://sqlserver:1433;databaseName=smartpool;encrypt=true;trustServerCertificate=true;\n"
     ]
    }
   ],
   "source": [
    "# from smartpool_config import *\n",
    "\n",
    "# spark = create_spark(\"smartpool-kafka-sensors\")\n",
    "\n",
    "print(\"Spark OK:\", spark.version)\n",
    "print(\"BASE:\", BASE)\n",
    "print(\"BRONZE:\", BRONZE)\n",
    "print(\"SILVER:\", SILVER)\n",
    "print(\"GOLD:\", GOLD)\n",
    "print(\"STATE:\", STATE)\n",
    "print(\"JDBC:\", JDBC_URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a504faff-46f0-4d61-b2c7-fe96c849efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka\n",
    "KAFKA_BOOTSTRAP = \"192.168.1.40:9094\"\n",
    "TOPIC = \"smartpool-sensors\"\n",
    "\n",
    "# Medallion paths\n",
    "BRONZE_KAFKA = f\"{BRONZE}/smartpool_sensors_kafka\"\n",
    "SILVER_SENS  = f\"{SILVER}/smartpool_sensors\"\n",
    "GOLD_AGG     = f\"{GOLD}/smartpool_sensors_1m\"\n",
    "GOLD_ENR     = f\"{GOLD}/smartpool_sensors_enriched\"\n",
    "\n",
    "# Checkpoints\n",
    "CHK_BASE   = f\"{BASE}/_checkpoints\"\n",
    "BRONZE_CHK = f\"{CHK_BASE}/bronze_smartpool_sensors_kafka\"\n",
    "SILVER_CHK = f\"{CHK_BASE}/silver_smartpool_sensors\"\n",
    "GOLD_AGG_CHK = f\"{CHK_BASE}/gold_smartpool_sensors_1m\"\n",
    "GOLD_ENR_CHK = f\"{CHK_BASE}/gold_smartpool_sensors_enriched\"\n",
    "\n",
    "# SILVER\n",
    "SILVER_POOLS   = f\"{SILVER}/pools_dim\"\n",
    "SILVER_EVENTS  = f\"{SILVER}/maintenance_events\"\n",
    "SILVER_ELEC    = f\"{SILVER}/electricity_prices\"\n",
    "\n",
    "# GOLD\n",
    "GOLD_ELEC_DAILY = f\"{GOLD}/electricity_daily_stats\"\n",
    "GOLD_ELEC_PEAK  = f\"{GOLD}/electricity_peak_hours\"\n",
    "GOLD_EVENTS_ENR = f\"{GOLD}/maintenance_events_enriched\"\n",
    "GOLD_EVENTS_COST= f\"{GOLD}/maintenance_events_cost\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42cf0727-2eb7-4679-bc3b-2e1817b3abe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_exists(path: str) -> bool:\n",
    "    try:\n",
    "        return DeltaTable.isDeltaTable(spark, path)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def ensure_delta(path: str, schema: StructType, partition_cols=None):\n",
    "    if delta_exists(path):\n",
    "        return\n",
    "    w = (spark.createDataFrame([], schema).write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\"))\n",
    "    if partition_cols:\n",
    "        w = w.partitionBy(*partition_cols)\n",
    "    w.save(path)\n",
    "    print(\"Creada Delta vacía:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a8aed34-c74d-459d-aa8c-b53ebf8111d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/26 22:45:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creada Delta vacía: s3a://spark/medallion/bronze/smartpool_sensors_kafka\n",
      "BRONZE streaming -> s3a://spark/medallion/bronze/smartpool_sensors_kafka\n"
     ]
    }
   ],
   "source": [
    "raw = (\n",
    "    spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP)\n",
    "        .option(\"subscribe\", TOPIC)\n",
    "        .option(\"startingOffsets\", \"latest\") #DEBUG\n",
    "        .option(\"failOnDataLoss\", \"false\")\n",
    "        .option(\"maxOffsetsPerTrigger\", \"2000\")     # evita picos\n",
    "        .load()\n",
    ")\n",
    "\n",
    "bronze_schema = StructType([\n",
    "    StructField(\"topic\", StringType(), True),\n",
    "    StructField(\"partition\", IntegerType(), True),\n",
    "    StructField(\"offset\", LongType(), True),\n",
    "    StructField(\"kafka_ts\", TimestampType(), True),\n",
    "    StructField(\"kafka_key\", StringType(), True),\n",
    "    StructField(\"kafka_value\", StringType(), True),\n",
    "    StructField(\"ingest_ts\", TimestampType(), True),\n",
    "    StructField(\"ingest_date\", DateType(), True),\n",
    "])\n",
    "\n",
    "ensure_delta(BRONZE_KAFKA, bronze_schema, partition_cols=[\"ingest_date\"])\n",
    "\n",
    "bronze = (\n",
    "    raw.select(\n",
    "        F.col(\"topic\"),\n",
    "        F.col(\"partition\"),\n",
    "        F.col(\"offset\"),\n",
    "        F.col(\"timestamp\").alias(\"kafka_ts\"),\n",
    "        F.col(\"key\").cast(\"string\").alias(\"kafka_key\"),\n",
    "        F.col(\"value\").cast(\"string\").alias(\"kafka_value\"),\n",
    "        F.current_timestamp().alias(\"ingest_ts\"),\n",
    "        F.to_date(F.current_timestamp()).alias(\"ingest_date\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "q_bronze = (\n",
    "    bronze.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"path\", BRONZE_KAFKA)\n",
    "        .option(\"checkpointLocation\", BRONZE_CHK)\n",
    "        .partitionBy(\"ingest_date\")\n",
    "        .trigger(processingTime=\"10 seconds\")\n",
    "        .queryName(\"bronze_smartpool_sensors_kafka\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "print(\"BRONZE streaming ->\", BRONZE_KAFKA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2703f889-728f-48c3-a1af-44957d45677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creada Delta vacía: s3a://spark/medallion/silver/smartpool_sensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/26 22:46:02 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SILVER streaming -> s3a://spark/medallion/silver/smartpool_sensors\n"
     ]
    }
   ],
   "source": [
    "sensor_schema = StructType([\n",
    "    StructField(\"pool_id\", IntegerType(), True),\n",
    "    StructField(\"ts\", StringType(), True),  # viene ISO ms en el producer\n",
    "    StructField(\"ph\", DoubleType(), True),\n",
    "    StructField(\"chlorine_mg_l\", DoubleType(), True),\n",
    "    StructField(\"temp_c\", DoubleType(), True),\n",
    "    StructField(\"turbidity_ntu\", DoubleType(), True),\n",
    "    StructField(\"water_level_pct\", DoubleType(), True),\n",
    "    StructField(\"pump_kwh_est\", DoubleType(), True),  # opcional\n",
    "])\n",
    "\n",
    "silver_schema = StructType([\n",
    "    StructField(\"pool_id\", IntegerType(), True),\n",
    "    StructField(\"sensor_ts\", TimestampType(), True),\n",
    "    StructField(\"ph\", DoubleType(), True),\n",
    "    StructField(\"chlorine_mg_l\", DoubleType(), True),\n",
    "    StructField(\"temp_c\", DoubleType(), True),\n",
    "    StructField(\"turbidity_ntu\", DoubleType(), True),\n",
    "    StructField(\"water_level_pct\", DoubleType(), True),\n",
    "    StructField(\"pump_kwh_est\", DoubleType(), True),\n",
    "    StructField(\"ingest_date\", DateType(), True),\n",
    "    StructField(\"silver_ingest_ts\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "ensure_delta(SILVER_SENS, silver_schema, partition_cols=[\"ingest_date\"])\n",
    "\n",
    "bronze_stream = spark.readStream.format(\"delta\").load(BRONZE_KAFKA)\n",
    "\n",
    "parsed = (\n",
    "    bronze_stream\n",
    "        .select(\"kafka_value\", \"ingest_date\")\n",
    "        .withColumn(\"json\", F.from_json(F.col(\"kafka_value\"), sensor_schema))\n",
    "        .select(\n",
    "            F.col(\"json.pool_id\").alias(\"pool_id\"),\n",
    "            F.col(\"json.ts\").alias(\"ts\"),\n",
    "            F.col(\"json.ph\").alias(\"ph\"),\n",
    "            F.col(\"json.chlorine_mg_l\").alias(\"chlorine_mg_l\"),\n",
    "            F.col(\"json.temp_c\").alias(\"temp_c\"),\n",
    "            F.col(\"json.turbidity_ntu\").alias(\"turbidity_ntu\"),\n",
    "            F.col(\"json.water_level_pct\").alias(\"water_level_pct\"),\n",
    "            F.col(\"json.pump_kwh_est\").alias(\"pump_kwh_est\"),\n",
    "            F.col(\"ingest_date\"),\n",
    "        )\n",
    "        # parse timestamp ISO: \"2026-01-25T19:07:21.820Z\"\n",
    "        # .withColumn(\"sensor_ts\", F.to_timestamp(F.regexp_replace(\"ts\", \"Z$\", \"\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\"))\n",
    "        .withColumn(\n",
    "            \"sensor_ts\",\n",
    "            F.coalesce(\n",
    "                F.to_timestamp(F.col(\"ts\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSX\"),     # 2026-01-25T22:38:45.553Z\n",
    "                F.to_timestamp(F.col(\"ts\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSX\"),  # 6 decimales + Z\n",
    "                F.to_timestamp(F.col(\"ts\"), \"yyyy-MM-dd'T'HH:mm:ssX\"),         # sin decimales + Z\n",
    "                F.to_timestamp(F.regexp_replace(\"ts\", \"Z$\", \"\"), \"yyyy-MM-dd'T'HH:mm:ss.SSS\"),    # fallback viejo\n",
    "                F.to_timestamp(F.regexp_replace(\"ts\", \"Z$\", \"\"), \"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\")  # fallback 6 decimales\n",
    "            )\n",
    "        )\n",
    "        .drop(\"ts\")\n",
    "        .withColumn(\"silver_ingest_ts\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Reglas básicas de calidad (ajusta si quieres)\n",
    "silver = (parsed\n",
    "    .filter(F.col(\"pool_id\").isNotNull())\n",
    "    .filter(F.col(\"sensor_ts\").isNotNull())\n",
    "    .filter(F.col(\"ph\").between(0.0, 14.0))\n",
    "    .filter(F.col(\"chlorine_mg_l\").between(0.0, 10.0))\n",
    "    .filter(F.col(\"temp_c\").between(-5.0, 60.0))\n",
    "    .filter(F.col(\"turbidity_ntu\").between(0.0, 200.0))\n",
    "    .filter(F.col(\"water_level_pct\").between(0.0, 100.0))\n",
    ")\n",
    "\n",
    "q_silver = (\n",
    "    silver.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"path\", SILVER_SENS)\n",
    "        .option(\"checkpointLocation\", SILVER_CHK)\n",
    "        .partitionBy(\"ingest_date\")\n",
    "        .trigger(processingTime=\"10 seconds\")\n",
    "        .queryName(\"silver_smartpool_sensors\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "print(\"SILVER streaming ->\", SILVER_SENS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7dd65fd-9918-452a-9662-569e05dd7714",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creada Delta vacía: s3a://spark/medallion/gold/smartpool_sensors_1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=>                (4 + 4) / 50][Stage 5:>                 (0 + 0) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOLD agg streaming -> s3a://spark/medallion/gold/smartpool_sensors_1m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=====>           (16 + 4) / 50][Stage 5:>                 (0 + 0) / 50]\r"
     ]
    }
   ],
   "source": [
    "silver_stream = spark.readStream.format(\"delta\").load(SILVER_SENS)\n",
    "\n",
    "# umbrales “smartpool” (puedes alinear con tu propuesta)\n",
    "PH_MIN, PH_MAX = 7.1, 7.8\n",
    "CL_MIN, CL_MAX = 0.4, 1.5\n",
    "\n",
    "gold_agg = (\n",
    "    silver_stream\n",
    "        # .withWatermark(\"sensor_ts\", \"2 minutes\")\n",
    "        .withWatermark(\"sensor_ts\", \"10 seconds\") #DEBUG\n",
    "        .groupBy(\n",
    "            F.window(F.col(\"sensor_ts\"), \"1 minute\").alias(\"w\"),\n",
    "            F.col(\"pool_id\")\n",
    "        )\n",
    "        .agg(\n",
    "            F.avg(\"ph\").alias(\"ph_avg\"),\n",
    "            F.avg(\"chlorine_mg_l\").alias(\"chlorine_avg\"),\n",
    "            F.avg(\"temp_c\").alias(\"temp_avg\"),\n",
    "            F.max(\"turbidity_ntu\").alias(\"turbidity_max\"),\n",
    "            F.avg(\"water_level_pct\").alias(\"water_level_avg\"),\n",
    "            F.sum(F.coalesce(\"pump_kwh_est\", F.lit(0.0))).alias(\"pump_kwh_sum\"),\n",
    "            F.count(\"*\").alias(\"num_readings\")\n",
    "        )\n",
    "        .select(\n",
    "            \"pool_id\",\n",
    "            F.col(\"w.start\").alias(\"window_start\"),\n",
    "            F.col(\"w.end\").alias(\"window_end\"),\n",
    "            \"ph_avg\",\"chlorine_avg\",\"temp_avg\",\"turbidity_max\",\"water_level_avg\",\n",
    "            \"pump_kwh_sum\",\"num_readings\",\n",
    "            # flags\n",
    "            ( (F.col(\"ph_avg\") < PH_MIN) | (F.col(\"ph_avg\") > PH_MAX) ).alias(\"ph_out_of_range\"),\n",
    "            ( (F.col(\"chlorine_avg\") < CL_MIN) | (F.col(\"chlorine_avg\") > CL_MAX) ).alias(\"chlorine_out_of_range\"),\n",
    "            F.current_timestamp().alias(\"calc_ts\"),\n",
    "            F.to_date(F.current_timestamp()).alias(\"calc_date\"),\n",
    "        )\n",
    ")\n",
    "\n",
    "# crear si no existe\n",
    "gold_agg_schema = StructType([\n",
    "    StructField(\"pool_id\", IntegerType(), True),\n",
    "    StructField(\"window_start\", TimestampType(), True),\n",
    "    StructField(\"window_end\", TimestampType(), True),\n",
    "    StructField(\"ph_avg\", DoubleType(), True),\n",
    "    StructField(\"chlorine_avg\", DoubleType(), True),\n",
    "    StructField(\"temp_avg\", DoubleType(), True),\n",
    "    StructField(\"turbidity_max\", DoubleType(), True),\n",
    "    StructField(\"water_level_avg\", DoubleType(), True),\n",
    "    StructField(\"pump_kwh_sum\", DoubleType(), True),\n",
    "    StructField(\"num_readings\", LongType(), True),\n",
    "    StructField(\"ph_out_of_range\", BooleanType(), True),\n",
    "    StructField(\"chlorine_out_of_range\", BooleanType(), True),\n",
    "    StructField(\"calc_ts\", TimestampType(), True),\n",
    "    StructField(\"calc_date\", DateType(), True),\n",
    "])\n",
    "\n",
    "ensure_delta(GOLD_AGG, gold_agg_schema, partition_cols=[\"calc_date\"])\n",
    "\n",
    "q_gold_agg = (\n",
    "    gold_agg.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"path\", GOLD_AGG)\n",
    "        .option(\"checkpointLocation\", GOLD_AGG_CHK)\n",
    "        .partitionBy(\"calc_date\")\n",
    "        .trigger(processingTime=\"10 seconds\")\n",
    "        .queryName(\"gold_smartpool_sensors_1m\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "print(\"GOLD agg streaming ->\", GOLD_AGG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "142211c4-8339-49e0-a3f2-50f4a338933e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Cannot start query with name gold_smartpool_sensors_enriched as a query with that name is already active in this SparkSession",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 56\u001b[0m\n\u001b[1;32m     39\u001b[0m     (spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([], enr_schema)\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwriteSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalc_date\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;241m.\u001b[39msave(GOLD_ENR)\n\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreada GOLD enriched:\u001b[39m\u001b[38;5;124m\"\u001b[39m, GOLD_ENR)\n\u001b[1;32m     47\u001b[0m q_gold_enr \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     48\u001b[0m     \u001b[43menriched\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdelta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGOLD_ENR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpointLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mGOLD_ENR_CHK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcalc_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrigger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessingTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m10 seconds\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqueryName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgold_smartpool_sensors_enriched\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 56\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGOLD enriched streaming ->\u001b[39m\u001b[38;5;124m\"\u001b[39m, GOLD_ENR)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/streaming/readwriter.py:1704\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[1;32m   1702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1705\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Cannot start query with name gold_smartpool_sensors_enriched as a query with that name is already active in this SparkSession"
     ]
    }
   ],
   "source": [
    "# Batch dims (estáticos para el streaming)\n",
    "pools_dim = (spark.read.format(\"delta\").load(SILVER_POOLS)\n",
    "    .select(\"pool_id\", \"pool_name\", \"location\", \"owner_type\", \"is_heated\", \"volume_liters\")\n",
    "    .dropDuplicates([\"pool_id\"])\n",
    ")\n",
    "\n",
    "elec = (spark.read.format(\"delta\").load(SILVER_ELEC)\n",
    "    .select(\n",
    "        F.col(\"date\").alias(\"elec_date\"),\n",
    "        F.col(\"hour\").alias(\"elec_hour\"),\n",
    "        \"region\",\n",
    "        \"price_eur_kwh\",\n",
    "        \"price_eur_mwh\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Enriquecemos lecturas (no la agregación), para tener coste puntual\n",
    "silver_stream2 = spark.readStream.format(\"delta\").load(SILVER_SENS)\n",
    "\n",
    "enriched = (silver_stream2\n",
    "    .withColumn(\"event_date\", F.to_date(\"sensor_ts\"))\n",
    "    .withColumn(\"event_hour\", F.hour(\"sensor_ts\"))\n",
    "    .join(pools_dim, on=\"pool_id\", how=\"left\")\n",
    "    .join(\n",
    "        elec,\n",
    "        (F.col(\"event_date\") == F.col(\"elec_date\")) & (F.col(\"event_hour\") == F.col(\"elec_hour\")),\n",
    "        how=\"left\"\n",
    "    )\n",
    "    .withColumn(\"pump_kwh_est\", F.coalesce(F.col(\"pump_kwh_est\"), F.lit(0.0)))\n",
    "    .withColumn(\"pump_cost_eur_est\", F.col(\"pump_kwh_est\") * F.col(\"price_eur_kwh\"))\n",
    "    .withColumn(\"calc_ts\", F.current_timestamp())\n",
    "    .withColumn(\"calc_date\", F.to_date(F.col(\"calc_ts\")))\n",
    "    .drop(\"elec_date\", \"elec_hour\")\n",
    ")\n",
    "\n",
    "# Crea tabla GOLD_ENR si no existe\n",
    "enr_schema = enriched.schema  # ok para crear vacío\n",
    "if not delta_exists(GOLD_ENR):\n",
    "    (spark.createDataFrame([], enr_schema).write.format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .partitionBy(\"calc_date\")\n",
    "        .save(GOLD_ENR)\n",
    "    )\n",
    "    print(\"Creada GOLD enriched:\", GOLD_ENR)\n",
    "\n",
    "q_gold_enr = (\n",
    "    enriched.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"path\", GOLD_ENR)\n",
    "        .option(\"checkpointLocation\", GOLD_ENR_CHK)\n",
    "        .partitionBy(\"calc_date\")\n",
    "        .trigger(processingTime=\"10 seconds\")\n",
    "        .queryName(\"gold_smartpool_sensors_enriched\")\n",
    "        .start()\n",
    ")\n",
    "\n",
    "print(\"GOLD enriched streaming ->\", GOLD_ENR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ab0343f-ab71-43ea-a4a6-e61781a8f713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streams activos: 4\n",
      "- gold_smartpool_sensors_1m 96e72db3-decb-4372-90d2-3d670f40b46f | isActive: True\n",
      "- gold_smartpool_sensors_enriched c9a5e626-6623-4ce9-8cff-90ee90e6a1d5 | isActive: True\n",
      "- silver_smartpool_sensors 39af24f3-8edf-48d5-b46a-1575ed45d100 | isActive: True\n",
      "- bronze_smartpool_sensors_kafka 4f82ae83-5f51-4a54-9fc8-b8dabdcf4bd5 | isActive: True\n",
      "\n",
      "== Progreso (lastProgress) ==\n",
      "[gold_smartpool_sensors_1m] batchId=52 inputRows=0 procRPS=0.00 watermark=2026-01-26T22:54:09.810Z\n",
      "[gold_smartpool_sensors_enriched] batchId=49 inputRows=20 procRPS=1.93 watermark=None\n",
      "[silver_smartpool_sensors] batchId=51 inputRows=10 procRPS=1.49 watermark=None\n",
      "[bronze_smartpool_sensors_kafka] batchId=52 inputRows=10 procRPS=2.57 watermark=None\n",
      "\n",
      "== Preview GOLD_1m (batch read) ==\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+-------------+-----------------+------------------+------------+---------------+---------------------+-----------------------+----------+\n",
      "|pool_id|window_start       |window_end         |ph_avg            |chlorine_avg      |temp_avg          |turbidity_max|water_level_avg  |pump_kwh_sum      |num_readings|ph_out_of_range|chlorine_out_of_range|calc_ts                |calc_date |\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+-------------+-----------------+------------------+------------+---------------+---------------------+-----------------------+----------+\n",
      "|2      |2026-01-26 23:53:00|2026-01-26 23:54:00|7.35              |1.05              |24.754            |1.78         |82.44399999999999|2.081             |5           |false          |false                |2026-01-26 23:54:41.364|2026-01-26|\n",
      "|6      |2026-01-26 23:53:00|2026-01-26 23:54:00|7.372222222222222 |1.008888888888889 |23.754444444444445|1.62         |86.55222222222223|2.58              |9           |false          |false                |2026-01-26 23:54:41.364|2026-01-26|\n",
      "|5      |2026-01-26 23:53:00|2026-01-26 23:54:00|7.46              |0.9984615384615385|23.681538461538462|1.97         |86.76846153846154|4.095000000000001 |13          |false          |false                |2026-01-26 23:54:41.364|2026-01-26|\n",
      "|1      |2026-01-26 23:53:00|2026-01-26 23:54:00|7.603333333333333 |1.0441666666666667|23.55666666666667 |1.76         |86.14999999999999|3.0620000000000003|12          |false          |false                |2026-01-26 23:54:41.364|2026-01-26|\n",
      "|3      |2026-01-26 23:53:00|2026-01-26 23:54:00|7.286             |1.376             |25.786            |1.82         |88.13            |0.8270000000000001|5           |false          |false                |2026-01-26 23:54:41.364|2026-01-26|\n",
      "|4      |2026-01-26 23:53:00|2026-01-26 23:54:00|7.374375000000001 |1.0375            |24.319375         |1.99         |84.929375        |4.511             |16          |false          |false                |2026-01-26 23:54:41.364|2026-01-26|\n",
      "|4      |2026-01-26 23:52:00|2026-01-26 23:53:00|7.447272727272727 |0.9599999999999999|25.530000000000005|1.96         |87.69454545454546|2.814             |11          |false          |false                |2026-01-26 23:53:51.132|2026-01-26|\n",
      "|2      |2026-01-26 23:52:00|2026-01-26 23:53:00|7.527692307692307 |1.2215384615384617|22.970000000000002|1.86         |84.14            |3.5660000000000003|13          |false          |false                |2026-01-26 23:53:51.132|2026-01-26|\n",
      "|5      |2026-01-26 23:52:00|2026-01-26 23:53:00|7.593571428571428 |1.2128571428571429|25.125714285714285|1.95         |90.59142857142858|3.824             |14          |false          |false                |2026-01-26 23:53:51.132|2026-01-26|\n",
      "|3      |2026-01-26 23:52:00|2026-01-26 23:53:00|7.405             |1.0266666666666666|21.353333333333335|1.65         |85.10166666666666|2.392             |6           |false          |false                |2026-01-26 23:53:51.132|2026-01-26|\n",
      "|1      |2026-01-26 23:52:00|2026-01-26 23:53:00|7.44888888888889  |1.221111111111111 |26.39666666666667 |1.9          |87.86666666666666|2.8040000000000003|9           |false          |false                |2026-01-26 23:53:51.132|2026-01-26|\n",
      "|6      |2026-01-26 23:52:00|2026-01-26 23:53:00|7.3757142857142854|1.0585714285714285|25.517142857142854|1.96         |83.57857142857142|2.651             |7           |false          |false                |2026-01-26 23:53:51.132|2026-01-26|\n",
      "|2      |2026-01-26 23:51:00|2026-01-26 23:52:00|7.443333333333334 |1.0916666666666666|21.763333333333335|1.54         |84.955           |1.823             |6           |false          |false                |2026-01-26 23:52:51.04 |2026-01-26|\n",
      "|5      |2026-01-26 23:51:00|2026-01-26 23:52:00|7.359             |1.199             |24.253            |1.83         |87.69            |2.272             |10          |false          |false                |2026-01-26 23:52:51.04 |2026-01-26|\n",
      "|3      |2026-01-26 23:51:00|2026-01-26 23:52:00|7.529999999999999 |0.9680000000000002|23.088            |1.65         |85.68700000000001|3.351             |10          |false          |false                |2026-01-26 23:52:51.04 |2026-01-26|\n",
      "|6      |2026-01-26 23:51:00|2026-01-26 23:52:00|7.4725            |0.975             |24.4925           |1.63         |87.30499999999999|1.369             |8           |false          |false                |2026-01-26 23:52:51.04 |2026-01-26|\n",
      "|4      |2026-01-26 23:51:00|2026-01-26 23:52:00|7.527777777777778 |0.8833333333333333|24.445555555555558|1.87         |88.24555555555555|2.365             |9           |false          |false                |2026-01-26 23:52:51.04 |2026-01-26|\n",
      "|1      |2026-01-26 23:51:00|2026-01-26 23:52:00|7.378823529411765 |1.0641176470588236|24.108235294117645|1.96         |79.71            |4.926             |17          |false          |false                |2026-01-26 23:52:51.04 |2026-01-26|\n",
      "|3      |2026-01-26 23:50:00|2026-01-26 23:51:00|7.505454545454544 |0.938181818181818 |22.900909090909092|1.97         |84.34            |2.739             |11          |false          |false                |2026-01-26 23:51:50.866|2026-01-26|\n",
      "|5      |2026-01-26 23:50:00|2026-01-26 23:51:00|7.675000000000001 |0.7849999999999999|26.03             |2.0          |91.29124999999999|1.9069999999999998|8           |false          |false                |2026-01-26 23:51:50.866|2026-01-26|\n",
      "|4      |2026-01-26 23:50:00|2026-01-26 23:51:00|7.496923076923077 |1.3007692307692307|25.870769230769234|1.98         |81.92153846153846|2.0839999999999996|13          |false          |false                |2026-01-26 23:51:50.866|2026-01-26|\n",
      "|1      |2026-01-26 23:50:00|2026-01-26 23:51:00|7.465384615384614 |0.9946153846153846|23.503076923076925|1.97         |84.31769230769231|3.306             |13          |false          |false                |2026-01-26 23:51:50.866|2026-01-26|\n",
      "|2      |2026-01-26 23:50:00|2026-01-26 23:51:00|7.234285714285713 |1.0185714285714285|26.15142857142857 |1.97         |87.59285714285714|2.575             |7           |false          |false                |2026-01-26 23:51:50.866|2026-01-26|\n",
      "|6      |2026-01-26 23:50:00|2026-01-26 23:51:00|7.4425            |1.4537499999999999|24.715            |1.66         |83.62124999999999|2.026             |8           |false          |false                |2026-01-26 23:51:50.866|2026-01-26|\n",
      "|6      |2026-01-26 23:49:00|2026-01-26 23:50:00|7.424285714285715 |1.1142857142857143|20.27             |1.97         |90.40714285714286|2.7030000000000003|7           |false          |false                |2026-01-26 23:50:50.882|2026-01-26|\n",
      "|3      |2026-01-26 23:49:00|2026-01-26 23:50:00|7.685714285714285 |1.0450000000000002|23.853571428571424|1.59         |85.06357142857142|4.863             |14          |false          |false                |2026-01-26 23:50:50.882|2026-01-26|\n",
      "|1      |2026-01-26 23:49:00|2026-01-26 23:50:00|7.353333333333333 |0.9677777777777778|22.64             |1.52         |86.04555555555557|3.4250000000000007|9           |false          |false                |2026-01-26 23:50:50.882|2026-01-26|\n",
      "|4      |2026-01-26 23:49:00|2026-01-26 23:50:00|7.409999999999999 |1.131             |22.024            |1.6          |84.742           |3.525             |10          |false          |false                |2026-01-26 23:50:50.882|2026-01-26|\n",
      "|5      |2026-01-26 23:49:00|2026-01-26 23:50:00|7.529999999999999 |0.86              |23.39             |1.92         |85.63111111111112|2.4000000000000004|9           |false          |false                |2026-01-26 23:50:50.882|2026-01-26|\n",
      "|2      |2026-01-26 23:49:00|2026-01-26 23:50:00|7.347272727272727 |1.1363636363636362|25.438181818181818|1.83         |88.45636363636365|3.0170000000000003|11          |false          |false                |2026-01-26 23:50:50.882|2026-01-26|\n",
      "|6      |2026-01-26 23:48:00|2026-01-26 23:49:00|7.47090909090909  |0.9954545454545454|24.732727272727274|1.81         |85.28181818181817|2.4250000000000003|11          |false          |false                |2026-01-26 23:49:50.631|2026-01-26|\n",
      "|3      |2026-01-26 23:48:00|2026-01-26 23:49:00|7.5025            |1.2808333333333333|22.734166666666667|1.69         |87.05583333333334|2.4939999999999998|12          |false          |false                |2026-01-26 23:49:50.631|2026-01-26|\n",
      "|1      |2026-01-26 23:48:00|2026-01-26 23:49:00|7.465999999999999 |1.034             |23.102            |1.89         |81.85600000000001|1.8969999999999998|5           |false          |false                |2026-01-26 23:49:50.631|2026-01-26|\n",
      "|5      |2026-01-26 23:48:00|2026-01-26 23:49:00|7.708571428571427 |1.0278571428571428|23.703571428571426|1.92         |90.11928571428572|3.797             |14          |false          |false                |2026-01-26 23:49:50.631|2026-01-26|\n",
      "|4      |2026-01-26 23:48:00|2026-01-26 23:49:00|7.464444444444443 |1.0944444444444443|23.771111111111114|1.81         |88.63777777777777|3.026             |9           |false          |false                |2026-01-26 23:49:50.631|2026-01-26|\n",
      "|2      |2026-01-26 23:48:00|2026-01-26 23:49:00|7.553333333333332 |1.4400000000000002|24.058888888888887|1.91         |85.1211111111111 |3.468             |9           |false          |false                |2026-01-26 23:49:50.631|2026-01-26|\n",
      "|4      |2026-01-26 23:47:00|2026-01-26 23:48:00|7.431666666666666 |0.9983333333333332|24.94916666666667 |1.91         |88.53916666666667|3.633             |12          |false          |false                |2026-01-26 23:48:50.445|2026-01-26|\n",
      "|1      |2026-01-26 23:47:00|2026-01-26 23:48:00|7.531999999999999 |0.8699999999999999|23.28             |1.84         |87.688           |3.0949999999999998|10          |false          |false                |2026-01-26 23:48:50.445|2026-01-26|\n",
      "|3      |2026-01-26 23:47:00|2026-01-26 23:48:00|7.527777777777778 |2.186666666666667 |25.44             |1.98         |84.34555555555556|2.5629999999999997|9           |false          |true                 |2026-01-26 23:48:50.445|2026-01-26|\n",
      "|2      |2026-01-26 23:47:00|2026-01-26 23:48:00|7.527142857142857 |1.542857142857143 |22.779999999999998|1.86         |79.87            |2.247             |7           |false          |true                 |2026-01-26 23:48:50.445|2026-01-26|\n",
      "|5      |2026-01-26 23:47:00|2026-01-26 23:48:00|7.527857142857143 |1.0314285714285716|22.900714285714287|1.69         |82.32785714285714|3.9970000000000003|14          |false          |false                |2026-01-26 23:48:50.445|2026-01-26|\n",
      "|6      |2026-01-26 23:47:00|2026-01-26 23:48:00|7.3825            |0.8375            |23.93             |1.61         |80.61625000000001|3.01              |8           |false          |false                |2026-01-26 23:48:50.445|2026-01-26|\n",
      "|5      |2026-01-26 23:46:00|2026-01-26 23:47:00|7.534285714285716 |1.4242857142857144|23.052857142857142|1.99         |83.55642857142855|4.176             |14          |false          |false                |2026-01-26 23:47:51.674|2026-01-26|\n",
      "|3      |2026-01-26 23:46:00|2026-01-26 23:47:00|7.050000000000001 |1.2544444444444445|22.59777777777778 |1.8          |90.91777777777777|2.436             |9           |true           |false                |2026-01-26 23:47:51.674|2026-01-26|\n",
      "|4      |2026-01-26 23:46:00|2026-01-26 23:47:00|7.295             |1.336             |24.427999999999997|1.8          |81.491           |3.845             |10          |false          |false                |2026-01-26 23:47:51.674|2026-01-26|\n",
      "|2      |2026-01-26 23:46:00|2026-01-26 23:47:00|7.401999999999999 |1.025             |25.149            |1.79         |84.242           |3.828             |10          |false          |false                |2026-01-26 23:47:51.674|2026-01-26|\n",
      "|6      |2026-01-26 23:46:00|2026-01-26 23:47:00|7.413333333333333 |0.9922222222222222|23.26111111111111 |1.14         |85.1711111111111 |1.888             |9           |false          |false                |2026-01-26 23:47:51.674|2026-01-26|\n",
      "|1      |2026-01-26 23:46:00|2026-01-26 23:47:00|7.491428571428571 |1.0328571428571427|22.794285714285714|1.82         |79.87428571428572|1.981             |7           |false          |false                |2026-01-26 23:47:51.674|2026-01-26|\n",
      "+-------+-------------------+-------------------+------------------+------------------+------------------+-------------+-----------------+------------------+------------+---------------+---------------------+-----------------------+----------+\n",
      "\n",
      "\n",
      "== Preview SILVER (batch read) ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/26 22:55:06 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 16638 milliseconds\n",
      "[Stage 2170:>               (0 + 4) / 6][Stage 2174:>              (0 + 0) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se pudo leer SILVER: [Errno 111] Connection refused\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 43426)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Streams activos:\", len(spark.streams.active))\n",
    "for q in spark.streams.active:\n",
    "    print(\"-\", q.name, q.id, \"| isActive:\", q.isActive)\n",
    "\n",
    "def print_progress(q):\n",
    "    lp = q.lastProgress\n",
    "    if not lp:\n",
    "        print(f\"[{q.name}] sin lastProgress todavía\")\n",
    "        return\n",
    "    evt = lp.get(\"eventTime\", {})\n",
    "    print(\n",
    "        f\"[{q.name}] batchId={lp.get('batchId')} \"\n",
    "        f\"inputRows={lp.get('numInputRows')} \"\n",
    "        f\"procRPS={lp.get('processedRowsPerSecond'):.2f} \"\n",
    "        f\"watermark={evt.get('watermark')}\"\n",
    "    )\n",
    "\n",
    "print(\"\\n== Progreso (lastProgress) ==\")\n",
    "for q in spark.streams.active:\n",
    "    print_progress(q)\n",
    "\n",
    "print(\"\\n== Preview GOLD_1m (batch read) ==\")\n",
    "try:\n",
    "    (spark.read.format(\"delta\")\n",
    "        .load(GOLD_AGG)\n",
    "        .orderBy(F.col(\"window_start\").desc())\n",
    "        .show(50, truncate=False))\n",
    "except Exception as e:\n",
    "    print(\"No se pudo leer GOLD_1m:\", e)\n",
    "\n",
    "print(\"\\n== Preview SILVER (batch read) ==\")\n",
    "try:\n",
    "    (spark.read.format(\"delta\")\n",
    "        .load(SILVER_SENS)\n",
    "        .orderBy(F.col(\"sensor_ts\").desc())\n",
    "        .show(20, truncate=False))\n",
    "except Exception as e:\n",
    "    print(\"No se pudo leer SILVER:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01e6a4b7-b204-491d-a332-b9a3bfae2a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Streams activos:\", len(spark.streams.active))\n",
    "# for q in spark.streams.active:\n",
    "#     print(\"-\", q.name, q.id)\n",
    "\n",
    "# # Preview por consola (opcional). Si te consume, comenta esta parte.\n",
    "# console_q = (\n",
    "#     gold_agg.writeStream\n",
    "#         .format(\"console\")\n",
    "#         .outputMode(\"append\")\n",
    "#         .option(\"truncate\", \"false\")\n",
    "#         .option(\"checkpointLocation\", f\"{CHK_BASE}/console_smartpool_sensors_preview\")\n",
    "#         .trigger(processingTime=\"10 seconds\")\n",
    "#         .queryName(\"console_smartpool_sensors_preview\")\n",
    "#         .start()\n",
    "# )\n",
    "\n",
    "# spark.streams.awaitAnyTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
