FROM apache/airflow:3.0.6

USER root
RUN apt-get update && apt-get install -y openjdk-17-jdk

ENV JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

ENV SPARK_VERSION=4.0.1
ENV HADOOP_VERSION=3

RUN curl -fSL https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \
    | tar -xz -C /opt

ENV SPARK_HOME=/opt/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}
ENV PATH=$PATH:$SPARK_HOME/bin

ARG HADOOP_AWS_VER=3.4.0
ARG AWS_SDK_BUNDLE_VER=1.12.262
ARG SPARK_HOME=/opt/spark
ARG SCALA_BINARY=2.13
ARG DELTA_VERSION=4.0.0

USER root

# Install Python 3.11 and required tools
# RUN apt-get update && \
#     apt-get install -y software-properties-common && \
#     add-apt-repository ppa:deadsnakes/ppa && \
#     apt-get update && \
#     apt-get install -y python3.11 python3.11-distutils python3.11-venv curl && \
#     rm -rf /var/lib/apt/lists/*    

# # Install pip for Python 3.11
# RUN curl -sS https://bootstrap.pypa.io/get-pip.py | python3.11


# # Make python3.11 the default python & pip
# RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 && \
#     update-alternatives --install /usr/bin/pip pip /usr/local/bin/pip3.11 1

# ENV PATH="/usr/bin:${PATH}"    

# Actualiza pip (recomendado) e instala delta-spark 4.0.0
# RUN python3 -m pip install --upgrade pip  
# RUN pip install --no-cache-dir delta-spark==4.0.0


# MinIO (S3A)

ENV HADOOP_CONF_fs_s3a_impl=org.apache.hadoop.fs.s3a.S3AFileSystem
ENV HADOOP_CONF_fs_s3a_endpoint=s3.amazonaws.com

ENV SPARK_CONF_spark.hadoop.fs.s3a.endpoint=http://minio:9000
ENV SPARK_CONF_spark.hadoop.fs.s3a.path.style.access=true
ENV SPARK_CONF_spark.hadoop.fs.s3a.connection.ssl.enabled=false
ENV SPARK_CONF_spark.hadoop.fs.s3a.access.key=minioadmin
ENV SPARK_CONF_spark.hadoop.fs.s3a.secret.key=minioadmin123      
      # Cat√°logo en memoria (opcional: es el default si no hay Hive)
ENV SPARK_CONF_spark.sql.catalogImplementation=in-memory
      # (opcional) warehouse en S3A para `saveAsTable` no-hive
ENV SPARK_CONF_spark.sql.warehouse.dir=s3a://spark/warehouse  

# Environment variables so Spark executors use Python 3.11
ENV PYSPARK_PYTHON=python3.11
ENV PYSPARK_DRIVER_PYTHON=python3.11
ENV PATH="/opt/spark/bin:${PATH}"

RUN mkdir -p /opt/spark/jars
ENV PATH="/opt/spark/jars:${PATH}"    

ENV SPARK_EXTRA_CLASSPATH=/opt/spark/jars/*
# hadoop-COMMON
RUN curl -fL -o /opt/spark/jars/hadoop-common-${HADOOP_AWS_VER}.jar \
    "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-common/${HADOOP_AWS_VER}/hadoop-common-${HADOOP_AWS_VER}.jar"

# hadoop-aws
RUN curl -fL -o /opt/spark/jars/hadoop-aws-${HADOOP_AWS_VER}.jar \
    "https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/${HADOOP_AWS_VER}/hadoop-aws-${HADOOP_AWS_VER}.jar"

# AWS SDK Bundle
# RUN curl -fL -o /opt/spark/jars/aws-java-sdk-bundle-${AWS_SDK_BUNDLE_VER}.jar \
#     "https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar"

RUN curl -fL -o /opt/spark/jars/aws-java-sdk-bundle-2.23.19.jar \
    "https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.23.19/bundle-2.23.19.jar"


RUN curl -fL -o /opt/spark/jars/wildfly-openssl-1.1.3.Final.jar \
    "https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/1.1.3.Final/wildfly-openssl-1.1.3.Final.jar"    

 

#Delta Lake (Scala 2.13 para Spark 4.0.1)
RUN curl -fL -o /opt/spark/jars/delta-spark_${SCALA_BINARY}-${DELTA_VERSION}.jar \
    "https://repo1.maven.org/maven2/io/delta/delta-spark_${SCALA_BINARY}/${DELTA_VERSION}/delta-spark_${SCALA_BINARY}-${DELTA_VERSION}.jar"

RUN curl -fL -o /opt/spark/jars/delta-storage-${DELTA_VERSION}.jar \
    "https://repo1.maven.org/maven2/io/delta/delta-storage/${DELTA_VERSION}/delta-storage-${DELTA_VERSION}.jar"

RUN curl -fL -o /opt/spark/jars/antlr4-runtime-4.13.1.jar \
    "https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.13.1/antlr4-runtime-4.13.1.jar"

#kafka client
RUN curl -fL -o /opt/spark/jars/spark-sql-kafka-0-10_2.13-4.0.1.jar \
    "https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.13/4.0.1/spark-sql-kafka-0-10_2.13-4.0.1.jar"

RUN curl -fL -o /opt/spark/jars/kafka-clients-3.9.1.jar \
    "https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.9.1/kafka-clients-3.9.1.jar"    

RUN curl -fL -o /opt/spark/jars/commons-pool2-2.12.0.jar \
    "https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.12.0/commons-pool2-2.12.0.jar"

RUN curl -fL -o /opt/spark/jars/spark-token-provider-kafka-0-10_2.13-4.0.1.jar \
    "https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.13/4.0.1/spark-token-provider-kafka-0-10_2.13-4.0.1.jar"

RUN curl -fL -o /opt/spark/jars/scala-parallel-collections_2.13-1.2.0.jar \
    "https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parallel-collections_2.13/1.2.0/scala-parallel-collections_2.13-1.2.0.jar"

#JDBC for MSSQL
RUN curl -fL -o /opt/spark/jars/mssql-jdbc-12.10.2.jre11.jar \
    "https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/12.10.2.jre11/mssql-jdbc-12.10.2.jre11.jar"

RUN mkdir -p /opt/app/libs

ENV PATH="/opt/app/libs:${PATH}"   

RUN curl -fL -o /opt/app/libs/log4j-slf4j2-impl-2.25.2.jar \
    "https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-slf4j2-impl/2.25.2/log4j-slf4j2-impl-2.25.2.jar"

RUN curl -fL -o /opt/app/libs/log4j-core-2.25.2.jar \
    "https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.25.2/log4j-core-2.25.2.jar"   
USER airflow